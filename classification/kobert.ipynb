{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from pycaret.classification import setup, compare_models, evaluate_model, predict_model, finalize_model\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./dataset_1.csv', index_col = 0)\n",
    "df2 = pd.read_csv(',/dataset_2.csv')\n",
    "df2 = df2.drop_duplicates()\n",
    "data1 = df1[['keyword', 'label']].rename(columns = {'keyword' : 'content'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df['label'].value_counts()\n",
    "min_count = label_counts.min()\n",
    "sampling_count = min_count * 25\n",
    "\n",
    "def sample_data(group):\n",
    "    return group.sample(n=min(len(group), sampling_count))\n",
    "\n",
    "data = df.groupby('label', group_keys=False).apply(sample_data)\n",
    "data['label'].value_counts()\n",
    "data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)  \n",
    "        text = re.sub(r'\\W+', ' ', text)  \n",
    "        text = re.sub(r'\\s+', ' ', text).strip()  \n",
    "    else:\n",
    "        text = ''\n",
    "    return text\n",
    "\n",
    "\n",
    "data['content'] = data['content'].apply(preprocess_text)\n",
    "\n",
    "# 텍스트와 라벨\n",
    "texts = data['content'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "\n",
    "# 라벨 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# 데이터 오버샘플링\n",
    "ros = RandomOverSampler()\n",
    "texts_resampled, labels_resampled = ros.fit_resample(pd.DataFrame(texts, columns=['text']), pd.Series(labels_encoded))\n",
    "\n",
    "# 데이터셋 분할\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts_resampled['text'], labels_resampled, test_size=0.2, stratify=labels_resampled)\n",
    "\n",
    "# KoBERT 토크나이저 초기화\n",
    "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "\n",
    "# 데이터셋 전처리 함수\n",
    "def convert_data_to_examples(texts, labels):\n",
    "    input_examples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        input_examples.append(InputExample(guid=None, text_a=text, text_b=None, label=label))\n",
    "    return input_examples\n",
    "\n",
    "train_examples = convert_data_to_examples(train_texts, train_labels)\n",
    "val_examples = convert_data_to_examples(val_texts, val_labels)\n",
    "\n",
    "# 입력 포맷 변환\n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = []\n",
    "    for e in examples:\n",
    "        input_dict = tokenizer.encode_plus(e.text_a,\n",
    "                                           add_special_tokens=True,\n",
    "                                           max_length=max_length,\n",
    "                                           return_token_type_ids=True,\n",
    "                                           return_attention_mask=True,\n",
    "                                           padding='max_length',\n",
    "                                           truncation=True)\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = input_dict['input_ids'], input_dict['token_type_ids'], input_dict['attention_mask']\n",
    "        features.append(\n",
    "            InputFeatures(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label))\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    'input_ids': f.input_ids,\n",
    "                    'attention_mask': f.attention_mask,\n",
    "                    'token_type_ids': f.token_type_ids\n",
    "                }, f.label\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(gen,\n",
    "        ({\n",
    "            'input_ids': tf.int32,\n",
    "            'attention_mask': tf.int32,\n",
    "            'token_type_ids': tf.int32\n",
    "        }, tf.int64),\n",
    "        ({\n",
    "            'input_ids': tf.TensorShape([None]),\n",
    "            'attention_mask': tf.TensorShape([None]),\n",
    "            'token_type_ids': tf.TensorShape([None])\n",
    "        }, tf.TensorShape([]))\n",
    "    )\n",
    "\n",
    "train_dataset = convert_examples_to_tf_dataset(train_examples, tokenizer)\n",
    "val_dataset = convert_examples_to_tf_dataset(val_examples, tokenizer)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(100).batch(16).repeat(2)\n",
    "val_dataset = val_dataset.batch(16)\n",
    "\n",
    "# 모델 초기화\n",
    "model = TFBertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "model.fit(train_dataset, epochs=10, validation_data=val_dataset)\n",
    "\n",
    "\n",
    "model.save_pretrained('/newsurfing/kobert_news_classifier')\n",
    "tokenizer.save_pretrained('/newsurfing/kobert_news_classifier')\n",
    "\n",
    "np.save('/newsurfing/kobert_news_classifier/label_classes.npy', label_encoder.classes_)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
